{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ed55f8-b325-4e15-958f-a5e173995d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#from skimage import measure\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6544ab05-2a1a-46df-9ab0-3e2c8ba96a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDenoise(nn.Module):\n",
    "    #layers definition\n",
    "    def __init__(self): #constructor for uNetDenoise\n",
    "        super().__init__() #constructor of parent class to init inherited attributes\n",
    "        #Encoder portion, with 256x256x3 input. Padding: same for 3x3 convolutions\n",
    "\n",
    "        #1st downsample layer\n",
    "        self.conv11 = nn.Conv2d(2,64,kernel_size=3, padding=1) #dataset images are black and white\n",
    "        self.conv12 = nn.Conv2d(64,64, kernel_size=3, padding=1) #keep padding same, (kernselSize-1)/2\n",
    "        self.pool1=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #2nd downsample layer\n",
    "        self.conv21 = nn.Conv2d(64,128,kernel_size=3, padding=1)\n",
    "        self.conv22 = nn.Conv2d(128,128,kernel_size=3, padding=1)\n",
    "        self.pool2=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #3rd downsample layer\n",
    "        self.conv31 = nn.Conv2d(128,256,kernel_size=3, padding=1)\n",
    "        self.conv32 = nn.Conv2d(256,256,kernel_size=3, padding=1)\n",
    "        self.pool3=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #4th downsample layer\n",
    "        self.conv41 = nn.Conv2d(256,512,kernel_size=3, padding=1)\n",
    "        self.conv42 = nn.Conv2d(512,512,kernel_size=3, padding=1)\n",
    "        self.pool4=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #bottle neck\n",
    "        self.conv51 = nn.Conv2d(512,1024,kernel_size=3, padding=1)\n",
    "        self.conv52 = nn.Conv2d(1024,1024,kernel_size=3, padding=1)\n",
    "\n",
    "        #upsample 1\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2) #1024, but half \n",
    "        self.u11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.u12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        #upsample 2\n",
    "        self.upconv2 = nn.ConvTranspose2d(512,256,kernel_size=2,stride=2)\n",
    "        self.u21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.u22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        #upsample 3\n",
    "        self.upconv3 = nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)\n",
    "        self.u31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.u32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        #upsample 4\n",
    "        self.upconv4 = nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)\n",
    "        self.u41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.u42 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #encoder forward\n",
    "        xconv11 = relu(self.conv11(x))\n",
    "        xconv12 = relu(self.conv12(xconv11))\n",
    "        xpool1 = self.pool1(xconv12)\n",
    "\n",
    "        xconv21 = relu(self.conv21(xpool1))\n",
    "        xconv22 = relu(self.conv22(xconv21))\n",
    "        xpool2 = self.pool2(xconv22)\n",
    "\n",
    "        xconv31 = relu(self.conv31(xpool2))\n",
    "        xconv32 = relu(self.conv32(xconv31))\n",
    "        xpool3 = self.pool3(xconv32)\n",
    "\n",
    "        xconv41 = relu(self.conv41(xpool3))\n",
    "        xconv42 = relu(self.conv42(xconv41))\n",
    "        xpool4 = self.pool4(xconv42)\n",
    "\n",
    "        xconv51 = relu(self.conv51(xpool4))\n",
    "        xconv52 = relu(self.conv52(xconv51))\n",
    "\n",
    "        #decoder forward \n",
    "        xup1 = self.upconv1(xconv52)\n",
    "        xu11 = torch.cat([xup1, xconv42], dim=1)\n",
    "        xd11 = relu(self.u11(xu11))\n",
    "        xd12 = relu(self.u12(xd11))\n",
    "\n",
    "        xup2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xup2, xconv32], dim=1)\n",
    "        xd21 = relu(self.u21(xu22))\n",
    "        xd22 = relu(self.u22(xd21))\n",
    "\n",
    "        xup3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xup3, xconv22], dim=1)\n",
    "        xd31 = relu(self.u31(xu33))\n",
    "        xd32 = relu(self.u32(xd31))\n",
    "\n",
    "        xup4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xup4, xconv12], dim=1)\n",
    "        xd41 = relu(self.u41(xu44))\n",
    "        xd42 = relu(self.u42(xd41))\n",
    "\n",
    "        return xd42\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b914c7-cd0a-4f92-9865-5576ad5202f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 64, 256, 256]        1,216\n",
      "├─Conv2d: 1-2                            [-1, 64, 256, 256]        36,928\n",
      "├─MaxPool2d: 1-3                         [-1, 64, 128, 128]        --\n",
      "├─Conv2d: 1-4                            [-1, 128, 128, 128]       73,856\n",
      "├─Conv2d: 1-5                            [-1, 128, 128, 128]       147,584\n",
      "├─MaxPool2d: 1-6                         [-1, 128, 64, 64]         --\n",
      "├─Conv2d: 1-7                            [-1, 256, 64, 64]         295,168\n",
      "├─Conv2d: 1-8                            [-1, 256, 64, 64]         590,080\n",
      "├─MaxPool2d: 1-9                         [-1, 256, 32, 32]         --\n",
      "├─Conv2d: 1-10                           [-1, 512, 32, 32]         1,180,160\n",
      "├─Conv2d: 1-11                           [-1, 512, 32, 32]         2,359,808\n",
      "├─MaxPool2d: 1-12                        [-1, 512, 16, 16]         --\n",
      "├─Conv2d: 1-13                           [-1, 1024, 16, 16]        4,719,616\n",
      "├─Conv2d: 1-14                           [-1, 1024, 16, 16]        9,438,208\n",
      "├─ConvTranspose2d: 1-15                  [-1, 512, 32, 32]         2,097,664\n",
      "├─Conv2d: 1-16                           [-1, 512, 32, 32]         4,719,104\n",
      "├─Conv2d: 1-17                           [-1, 512, 32, 32]         2,359,808\n",
      "├─ConvTranspose2d: 1-18                  [-1, 256, 64, 64]         524,544\n",
      "├─Conv2d: 1-19                           [-1, 256, 64, 64]         1,179,904\n",
      "├─Conv2d: 1-20                           [-1, 256, 64, 64]         590,080\n",
      "├─ConvTranspose2d: 1-21                  [-1, 128, 128, 128]       131,200\n",
      "├─Conv2d: 1-22                           [-1, 128, 128, 128]       295,040\n",
      "├─Conv2d: 1-23                           [-1, 128, 128, 128]       147,584\n",
      "├─ConvTranspose2d: 1-24                  [-1, 64, 256, 256]        32,832\n",
      "├─Conv2d: 1-25                           [-1, 64, 256, 256]        73,792\n",
      "├─Conv2d: 1-26                           [-1, 1, 256, 256]         577\n",
      "==========================================================================================\n",
      "Total params: 30,994,753\n",
      "Trainable params: 30,994,753\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 52.19\n",
      "==========================================================================================\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 272.50\n",
      "Params size (MB): 118.24\n",
      "Estimated Total Size (MB): 391.24\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 64, 256, 256]        1,216\n",
       "├─Conv2d: 1-2                            [-1, 64, 256, 256]        36,928\n",
       "├─MaxPool2d: 1-3                         [-1, 64, 128, 128]        --\n",
       "├─Conv2d: 1-4                            [-1, 128, 128, 128]       73,856\n",
       "├─Conv2d: 1-5                            [-1, 128, 128, 128]       147,584\n",
       "├─MaxPool2d: 1-6                         [-1, 128, 64, 64]         --\n",
       "├─Conv2d: 1-7                            [-1, 256, 64, 64]         295,168\n",
       "├─Conv2d: 1-8                            [-1, 256, 64, 64]         590,080\n",
       "├─MaxPool2d: 1-9                         [-1, 256, 32, 32]         --\n",
       "├─Conv2d: 1-10                           [-1, 512, 32, 32]         1,180,160\n",
       "├─Conv2d: 1-11                           [-1, 512, 32, 32]         2,359,808\n",
       "├─MaxPool2d: 1-12                        [-1, 512, 16, 16]         --\n",
       "├─Conv2d: 1-13                           [-1, 1024, 16, 16]        4,719,616\n",
       "├─Conv2d: 1-14                           [-1, 1024, 16, 16]        9,438,208\n",
       "├─ConvTranspose2d: 1-15                  [-1, 512, 32, 32]         2,097,664\n",
       "├─Conv2d: 1-16                           [-1, 512, 32, 32]         4,719,104\n",
       "├─Conv2d: 1-17                           [-1, 512, 32, 32]         2,359,808\n",
       "├─ConvTranspose2d: 1-18                  [-1, 256, 64, 64]         524,544\n",
       "├─Conv2d: 1-19                           [-1, 256, 64, 64]         1,179,904\n",
       "├─Conv2d: 1-20                           [-1, 256, 64, 64]         590,080\n",
       "├─ConvTranspose2d: 1-21                  [-1, 128, 128, 128]       131,200\n",
       "├─Conv2d: 1-22                           [-1, 128, 128, 128]       295,040\n",
       "├─Conv2d: 1-23                           [-1, 128, 128, 128]       147,584\n",
       "├─ConvTranspose2d: 1-24                  [-1, 64, 256, 256]        32,832\n",
       "├─Conv2d: 1-25                           [-1, 64, 256, 256]        73,792\n",
       "├─Conv2d: 1-26                           [-1, 1, 256, 256]         577\n",
       "==========================================================================================\n",
       "Total params: 30,994,753\n",
       "Trainable params: 30,994,753\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 52.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.50\n",
       "Forward/backward pass size (MB): 272.50\n",
       "Params size (MB): 118.24\n",
       "Estimated Total Size (MB): 391.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = UNetDenoise()\n",
    "summary(model1, (2, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c2040a-d5c7-4835-83ae-efa0f5b40d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #using gpu\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eddad30-58b1-4ee9-b3c8-2d61148ed9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5ea17e-96dc-472b-bab9-1158a3c57ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform) #handles label creation, class names for imgs\n",
    "        \n",
    "    def __len__(self): #to let dataloader know len of data\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26a8215-b006-4f25-98b5-9759c5fc6631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanso\\Desktop\\projects_desktop\\UNetDenoiser\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21bb380c-395a-44b0-9f9b-2a733aad267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb17862-4676-4ff2-9b2e-04b2775f14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(lambda x: x/255) #normalization by div by 255\n",
    "    #transforms.Normalize(0,1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eafc950-d7e7-45ee-84be-48a0cc49cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanso\\Desktop\\projects_desktop\\UNetDenoiser\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214bd402-fd3c-4848-bc27-02a3ffd5f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'imgDataset/Train400/'\n",
    "test_dir = 'imgDataset/Test68/'\n",
    "train_datasetPreSplit =getDataset(train_dir, preprocess)\n",
    "test_dataset =getDataset(test_dir, preprocess)\n",
    "train_dataset, val_dataset = random_split(train_datasetPreSplit, [int(0.85*len(train_datasetPreSplit)), len(train_datasetPreSplit)-int(0.85*len(train_datasetPreSplit))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec107192-a433-49ac-93cf-3271df134937",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba06f500-4b53-4700-9081-8b5733e507be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN: 0.04313725605607033, Max: 0.8980392217636108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_dataset[1]\n",
    "print(f'MIN: {image.min()}, Max: {image.max()}')\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b57ab0-5954-45e6-9bf8-8afd11151b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "786ae842-5fb6-4f3c-a079-df2aed99a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=1e-3, weight_decay=1e-5)#adam adjustes the learning rate. weight decay for l2 reg. Use 1e-4, 1e-3 for lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7026b2e-849e-4b9a-802e-2db5bea7de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(mse, max):\n",
    "    return 10*np.log10((max**2)/mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd60be6e-4c95-42d9-a4d0-457a69431c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 50\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "model1.to(device)\n",
    "sigma = 15/255 #maybe try 15\n",
    "psnrMin = float('inf') #to keep track of smallest psnr val for saving wieghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39f5f40-f54c-403c-b154-d17db2126079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to keep track of losses and metrics at end of an epoch\n",
    "epochTrainLossList = list()\n",
    "epochValLossList = list()\n",
    "epochssimOutputList = list()\n",
    "epochssimNoisyList = list()\n",
    "epochpsnrNoisyList= list()\n",
    "epochpsnrOutputList = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c0a9cfa-f427-4321-9644-1aa5b59d7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise level map\n",
    "def genNoiseMap(image, sigmaNorm):\n",
    "    noiselvlMap = torch.full_like(image, sigmaNorm)\n",
    "    return noiselvlMap\n",
    "def combineImgandMap(imageNoised, noiselvlMap): #input image needs to be noised\n",
    "    combinedInput = torch.cat((imageNoised, noiselvlMap), dim=1)\n",
    "    return combinedInput\n",
    "def addNoiseMap(imageNoised, sigmaNorm): #image needs to be noised, sigma should be normalized?\n",
    "    noiselvlMap = genNoiseMap(imageNoised, sigmaNorm)\n",
    "    return combineImgandMap(imageNoised, noiselvlMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75c6d7-fe12-4229-902f-8deb38a10c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    #training\n",
    "    model1.train()\n",
    "    \n",
    "    curBatch = 0\n",
    "    trainLossList = list()\n",
    "    valLossList = list()\n",
    "   \n",
    "    \n",
    "    ssimOutputList = list()\n",
    "    ssimNoisyList = list()\n",
    "    psnrNoisyList= list()\n",
    "    psnrOutputList = list()\n",
    "\n",
    "     #image + noise, set sigma of noise(ex. 10/255), normalize the data. \n",
    "        #Then do clean_images+sigma*noisyimages\n",
    "        #check on validation set\n",
    "        #test set \n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        curBatch += 1\n",
    "        clean_images = images.to(device)\n",
    "\n",
    "        batchTrainLossList = list()\n",
    "        \n",
    "        for aCleanImg in clean_images:\n",
    "            noiseGenerated = torch.randn_like(clean_images).to(device) \n",
    "           \n",
    "            ###noise map implementation\n",
    "            noiseMapSigmaSampled = torch.randint(1,50,(1,1)).item()#each image have diff sigma\n",
    "            noiseMapSigma = noiseMapSigmaSampled/255\n",
    "            imgNoisyInput = aCleanImg+sigma*noiseGenerated #sigma and noise map sigma are different\n",
    "            modelInput = addNoiseMap(imgNoisyInput, noiseMapSigma)\n",
    "            ####\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model1(modelInput)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, aCleanImg)\n",
    "            #need to compare with clean image\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batchTrainLossList.append(loss.item())\n",
    "\n",
    "        trainLossList.append(sum(batchTrainLossList)/len(batchTrainLossList))\n",
    "\n",
    "        #validation should have 1 sigma value throguhout training loop\n",
    "        model1.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #sigma for noise map, fixed for all runs\n",
    "            noiseMapSigmaSampled = torch.randint(1,50,(1,1)).item()\n",
    "            noiseMapSigma = noiseMapSigmaSampled/255\n",
    "            \n",
    "            for images, labels in val_loader:\n",
    "\n",
    "                batchValLossList = list()\n",
    "                \n",
    "                clean_images = images.to(device)\n",
    "                noiseGenerated = torch.randn_like(clean_images).to(device) \n",
    "\n",
    "\n",
    "                ### noiseMap implementation\n",
    "                imgNoisyInput = clean_images+sigma*noiseGenerated  \n",
    "                modelInput = addNoiseMap(imgNoisyInput, noiseMapSigma)\n",
    "                \n",
    "                \n",
    "                for anCleanImage, anInput, aNoisyImage in zip(clean_images, modelInput, imgNoisyInput): #anInput is the nosiy image\n",
    "\n",
    "                    anOutput = model1(anInput)\n",
    "                    loss = criterion(anOutput, anCleanImage)\n",
    "                    batchValLossList.append(loss.item())\n",
    "\n",
    "                    # Convert tensors to numpy arrays and map values to [0, 1] range\n",
    "                    anCleanImage = (anCleanImage.cpu().detach().numpy())    #+1 then divide by 2 to make image pixels fall within range of SSIM.\n",
    "                    anOutput = (anOutput.cpu().detach().numpy())  #don't have to do these 3 lines if normalized by /255\n",
    "                    anInput = ((anInput.cpu().detach().numpy())).astype(np.float32)\n",
    "                    aNoisyImage = (aNoisyImage.cpu().detach().numpy())\n",
    "                    \n",
    "                    #calculating SSIM\n",
    "                    ssimOutputCalc = ssim(anCleanImage.squeeze(), anOutput.squeeze(), data_range=1)\n",
    "                    ssimNoisyCalc = ssim(anCleanImage.squeeze(), aNoisyImage.squeeze(), data_range=1) #just take channel of actual image for input\n",
    "                    ssimOutputList.append(ssimOutputCalc)\n",
    "                    ssimNoisyList.append(ssimNoisyCalc)\n",
    "\n",
    "                    #calculating MSE\n",
    "                    mseNoised = mse(anCleanImage.squeeze(), aNoisyImage.squeeze())\n",
    "                    mseOutput = mse(anCleanImage.squeeze(), anOutput.squeeze())\n",
    "                    #calc psnr\n",
    "                    psnrNoised = psnr(mseNoised, 1)\n",
    "                    psnrOutput=psnr(mseOutput, 1)\n",
    "                    psnrNoisyList.append(psnrNoised)\n",
    "                    psnrOutputList.append(psnrOutput)\n",
    "\n",
    "            valLossList.append(sum(batchValLossList)/len(batchValLossList))\n",
    "            print(f\" Batch: {curBatch} Of: {len(train_loader)}, Training Loss: {trainLossList[-1]:.4f}, Validation loss: {valLossList[-1]:.4f} Avg SSIM of noisy images: {sum(ssimNoisyList)/len(ssimNoisyList):.4f} Avg SSIM of output images: {sum(ssimOutputList)/len(ssimOutputList):.4f} | Avg PSNR of noisy images: {sum(psnrNoisyList)/len(psnrNoisyList):.4f}  Avg PSNR of output images: {sum(psnrOutputList)/len(psnrOutputList):.4f}\")\n",
    "\n",
    "\n",
    "                        \n",
    "    epochTrainLossList.append(sum(trainLossList)/len(trainLossList))\n",
    "    epochValLossList.append(sum(valLossList)/len(valLossList))\n",
    "    epochssimOutputList.append(sum(ssimOutputList)/len(ssimOutputList))\n",
    "    epochssimNoisyList.append(sum(ssimNoisyList)/len(ssimNoisyList))\n",
    "    epochpsnrNoisyList.append(sum(psnrNoisyList)/len(psnrNoisyList))\n",
    "    epochpsnrOutputList.append(sum(psnrOutputList)/len(psnrOutputList))\n",
    "\n",
    "    avgEpochPsnr = sum(psnrOutputList)/len(psnrOutputList)\n",
    "    #updating wieghts based on based psnr\n",
    "    if avgEpochPsnr < psnrMin:\n",
    "        torch.save(model1.state_dict(), \"denoiserModelBestWeightswNoiseMap.pth\")\n",
    "        psnrMin = avgEpochPsnr\n",
    "    #updating weights every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model1.state_dict(), \"denoiserWeightsCheckpointwNoiseMap.pth\") \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {sum(trainLossList)/len(trainLossList):.4f}, Validation Loss: {sum(valLossList)/len(valLossList):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f6260-953b-4ad0-82d3-9f02b975968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting PSNR\n",
    "numPsnrVals = len(epochpsnrOutputList)\n",
    "plt.plot(range(numPsnrVals), epochpsnrOutputList, label=\"denoised images\")\n",
    "plt.plot(range(numPsnrVals), epochpsnrNoisyList, label=\"noisy images\")\n",
    "plt.xlabel('PSNR overtime')\n",
    "plt.ylabel('PSNR value')\n",
    "plt.legend()\n",
    "plt.title('PSNR of Denoised Images (blue) vs PSNR of Noisy Images(orange)')\n",
    "plt.show()\n",
    "\n",
    "#plotting SSIM\n",
    "numSSIMVals = len(epochssimOutputList)\n",
    "plt.plot(range(numSSIMVals), epochssimOutputList, label=\"denoised images\")\n",
    "plt.plot(range(numSSIMVals), epochssimNoisyList, label=\"noisy images\")\n",
    "plt.xlabel('SSIM overtime')\n",
    "plt.ylabel('SSIM value')\n",
    "plt.legend()\n",
    "plt.title('SSIM of Denoised Images (blue) vs SSIM of Noisy Images(orange)')\n",
    "plt.show()\n",
    "\n",
    "#plotting Losses\n",
    "numEpochs = len(epochTrainLossList)\n",
    "plt.plot(range(numEpochs), epochTrainLossList, label=\"Training Loss\")\n",
    "plt.plot(range(numEpochs), epochValLossList, label=\"Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccf35e-78aa-4b44-8944-7c6f6cc0497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final weights save\n",
    "torch.save(model1.state_dict(), \"denoiserFinalWeightswNoiseMap.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32f504-9e9c-451b-be10-180f506e6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186c96e-c4f0-4841-98bf-3ab4cc2e5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "sigma=15/255\n",
    "noiseMapSigmaSampled = torch.randint(1,50,(1,1)).item()\n",
    "noiseMapSigma = noiseMapSigmaSampled/255\n",
    "for images, labels in test_loader:\n",
    "    imgNoisyInput = images+sigma*torch.randn_like(images) #adding more noise, imageswNoise denotes images with added noise\n",
    "    ###noise map implementation\n",
    "    imageNoised = addNoiseMap(imgNoisyInput, noiseMapSigma) #imageNoised denotes added noised image with noise map   \n",
    "    ###\n",
    "    #imagesNoised = images\n",
    "    imageNoised=imageNoised.to(device)\n",
    "    with torch.no_grad():\n",
    "        denoised_imgs = model1(imageNoised)\n",
    "    \n",
    "    # Convert tensors to numpy arrays and move to CPU\n",
    "    imageNoised = imageNoised.cpu().numpy()\n",
    "    denoised_imgs = denoised_imgs.cpu().numpy()\n",
    "    clean_imgsCpy = images #used to get 2 channels on clean image\n",
    "    clean_imgs = images.cpu().numpy()\n",
    "    imgNoisyInput = imgNoisyInput.cpu().numpy()\n",
    "    \n",
    "    num_imgs = len(imageNoised)\n",
    "    fig, axes = plt.subplots(num_imgs, 3, figsize=(7, num_imgs * 6))\n",
    "    \n",
    "    for i in range(num_imgs):\n",
    "        axes[i, 0].imshow(clean_imgs[i].squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title('Clean Test Image Input')\n",
    "        #axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(imageNoised[i][0].squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title('Image with added Noise')\n",
    "       # axes[i, 1].axis('off')\n",
    "        empty_channel = torch.zeros_like(clean_imgsCpy[i])\n",
    "        cleanImg2Chan = np.concatenate((clean_imgsCpy[i], empty_channel), axis=0)\n",
    "        \n",
    "        \n",
    "        ssimCalcNoised = ssim(clean_imgs[i].squeeze(), imgNoisyInput[i].squeeze(), data_range=1)\n",
    "        mseOutNoised = mse(imgNoisyInput[i].squeeze(), clean_imgs[i].squeeze()) #psnr between test(noisy) image and denoised\n",
    "        psnrOutNoised = psnr(mseOutNoised, 1)\n",
    "        axes[i, 1].set_xlabel(f'PSNR: {psnrOutNoised:.4f}, SSIM: {ssimCalcNoised:.4f}')\n",
    "        \n",
    "        # Plot the denoised output image\n",
    "        axes[i, 2].imshow(denoised_imgs[i].squeeze(), cmap='gray')\n",
    "        axes[i, 2].set_title('Denoised Output Image')\n",
    "        ssimCalcDenoised = ssim(clean_imgs[i].squeeze(), denoised_imgs[i].squeeze(), data_range=1)\n",
    "        mseOutDenoised = mse(denoised_imgs[i].squeeze(), clean_imgs[i].squeeze()) #psnr between test(noisy) image and denoised\n",
    "        psnrOutDenoised = psnr(mseOutDenoised, 1)\n",
    "        axes[i, 2].set_xlabel(f'PSNR: {psnrOutDenoised:.4f}, SSIM: {ssimCalcDenoised:.4f}')\n",
    "        #axes[i, 2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b96a0-cc22-4b18-8df3-acf386876de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
